# To-do 13: Poking at Big Data (Yelp)

## Na-Rae the fake student:
- My 5-year-old ThinkPad laptop did ok! It has ... RAM, which worked ok with this much data, etc. Grepping through the whole of xxx json file took...

## Wilson:
- My laptop has 16GB of RAM (although WSL2 only has access to 8GB of it), a 6-core Intel i7-8750H, and a 1TB SSD (with 223GB free.) 100k lines were handled fine, taking only 6.4s to process, but 1 million was too much; it terminated (due to maxing out the RAM) after about a minute. The fact that every line was being read into a dataframe seemed like the culprit, so I tried re-implementing the same functionality using streaming (with ijson) and it actually halved execution time *and* was able to process the *entire* file in just under 4 minutes. Now I know that if I hit resource limits doing something like this, I can try refactoring my code so it processes the data line-by-line as a stream rather than loading it all into a dataframe.

## Varun:
- I tried it on my Macbook Pro, 8 gigs of ram. It did not take it well when I attempted to read it all in at once. My computer fan started being extra noisy, my terminal went unresponsive and so did the rest of the computer, even to keyboard interrups. I managed to kill it with the classic `CTRL+\`, but had to restart my computer because nothing worked. Upon restarting, my wallpaper was changed for some reason. A part of the problem might be because of the 100 chrome tabs I that will never close (I need all of them). I tried again, specifying `chunksize`, and it took a few minutes but worked.

## Camryn:
- My laptopâ€™s handling of this dataset is about what I would expect it to be. I have a 2019 MacBook Pro with 2.4 GHz Quad-Core Intel Core i5 with 8 GB of RAM. Like most Macs, it gets overwhelmed just opening multiple tabs at once. I tested the script in increments of 10,000, then increments of 100,000, and then one increment of 1,000,000 just to see. While I ran the script with numbers below 100,000, the script ran incredibly fast and did not have much of an effect on my computer. After 100,000, but below 1,000,000, the script started taking longer to run but nothing more than a few seconds. Approaching 1,000,000 is when it started to slow down even further, and when it reached 1,000,000 it took around 3 minutes and 3 seconds to run. For fun, I tried running the script with 2,000,000 reviews and it took around 8 minutes before I gave up and terminated the process.  To run this in its entirety successfully and quickly, a computer with better hardware would definitely help.

## Mack
- I have a 2019 MacBook Pro with 1.4 GHz Quad-Core Intel Core i5 and 8GB of RAM. Running grep to look at 'horrible' and 'scrumptious' reviews took a lot of CPU resources and ran for a minute or two. I tried 10, 100, 500, 1,000, 5,000, 7,500, 10,000, 20,000, 50,000, and accidentally jumped up to 750,000 but even 750k ran in about a minute or so. The others all ran instantly or within a few seconds with very little effort from my CPU. Running 750K took a lot on the CPU but no major issues or sounds came up. I ran 1,000,000 lines and it ran in a minute and a half. Running the last test took up over 8GB of memory, so a computer with a larger memory will likely improve the processing speed of the data. I am sure that I could process more lines, but I did not want to push my computer further than trying a million lines.

## Moldir
- I use a 2017 MacBook Pro 14.2 with 3.1 GHz Dual-Core Intel Core i5 and 8GB of RAM. Running egrep functions in Step 1 to search for 'horrible', 'scrumptious', and 'delicious' tokens in the reviews file took a minute or so. However, checking word counts for each file took longer. In Step 2, I couldn't run a python script on a command line from the first attempt, and it took a while until I found how to reconfigure .bash_profile so it would execute python scripts. I tried 1K, 10K, 100K, and 1M sampling, and everything worked just fine up to 1M. The last sample took more than 5 minutes to run and used almost 92% of CPU resources. 
